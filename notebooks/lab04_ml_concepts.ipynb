{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 04: ML Concepts Primer\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/depalmar/ai_for_the_win/blob/main/notebooks/lab04_ml_concepts.ipynb)\n\nUnderstand machine learning theory before coding. No coding required - just concepts!\n\n## Learning Objectives\n- Supervised vs unsupervised learning\n- Features and labels\n- Training, validation, and testing\n- Evaluation metrics\n\n**Next:** Lab 31 (Prompt Engineering) or Lab 21 (Hello World ML)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Machine Learning?\n",
    "\n",
    "Machine learning is teaching computers to learn patterns from data instead of explicit programming.\n",
    "\n",
    "**Traditional Programming:**\n",
    "```\n",
    "Rules + Data \u2192 Program \u2192 Output\n",
    "```\n",
    "\n",
    "**Machine Learning:**\n",
    "```\n",
    "Data + Expected Output \u2192 ML Algorithm \u2192 Model (learned rules)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Supervised Learning\n\nLearning from labeled examples - **like studying with an answer key!**\n\n### \ud83d\udcda The Metaphor: Learning with a Teacher\n\nThink of supervised learning like a student studying for an exam with a study guide that has **questions AND answers**:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SUPERVISED LEARNING = Learning with an Answer Key              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502   Student sees:                        Teacher provides:        \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502 Email text   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 \"phishing\"   \u2502         \u2502\n\u2502   \u2502 with 5 URLs  \u2502                    \u2502              \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502 Email from   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 \"legitimate\" \u2502         \u2502\n\u2502   \u2502 known sender \u2502                    \u2502              \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                                 \u2502\n\u2502   After seeing 1000s of examples, the student learns:          \u2502\n\u2502   \"Lots of URLs + urgency = probably phishing!\"                \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### \ud83c\udfaf Real-World Analogy\n\n| Scenario | Input (Features) | Output (Label) |\n|----------|------------------|----------------|\n| Medical diagnosis | Symptoms, test results | Disease name |\n| Spam filter | Email text, sender | Spam / Not spam |\n| Malware detection | File behavior, imports | Malware / Benign |\n| Fraud detection | Transaction details | Fraudulent / Legitimate |\n\n**The key insight**: Someone had to manually label the training data first. In security, this often means analysts spending hours classifying threats!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of supervised learning\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "legitimate = np.random.randn(50, 2) + [2, 2]\n",
    "phishing = np.random.randn(50, 2) + [5, 5]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(legitimate[:, 0], legitimate[:, 1], c=\"green\", label=\"Legitimate\", alpha=0.7)\n",
    "plt.scatter(phishing[:, 0], phishing[:, 1], c=\"red\", label=\"Phishing\", alpha=0.7)\n",
    "plt.xlabel(\"Feature 1 (e.g., URL count)\")\n",
    "plt.ylabel(\"Feature 2 (e.g., urgency words)\")\n",
    "plt.title(\"Supervised Learning: Labeled Data\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Unsupervised Learning\n\nFinding patterns without labels - **like sorting a messy closet!**\n\n### \ud83d\udcda The Metaphor: Organizing Without Instructions\n\nImagine you're handed a box of 1000 random objects and told: \"Sort these into groups.\" No one tells you what the groups should be - you just find natural groupings yourself:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  UNSUPERVISED LEARNING = Organizing Without Instructions        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502   Before (raw data):           After (discovered clusters):    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 \ud83c\udfbe\u26bd\ud83c\udfc0\u26be\ud83c\udfb1\ud83d\udcf1     \u2502          \u2502 Sports:         \u2502             \u2502\n\u2502   \u2502 \ud83d\udcbb\ud83d\uddb1\ufe0f\ud83d\udcfa\ud83c\udfc8\ud83c\udfbf\ud83c\udfae     \u2502    \u2192     \u2502 \ud83c\udfbe\u26bd\ud83c\udfc0\u26be\ud83c\udfc8\ud83c\udfbf     \u2502             \u2502\n\u2502   \u2502 \ud83d\udda8\ufe0f\ud83d\udcfb\ud83d\udcf1\ud83c\udfbf\ud83d\udda5\ufe0f      \u2502          \u2502                 \u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502 Electronics:    \u2502             \u2502\n\u2502                                \u2502 \ud83d\udcf1\ud83d\udcbb\ud83d\uddb1\ufe0f\ud83d\udcfa\ud83c\udfae\ud83d\udda8\ufe0f\ud83d\udcfb\ud83d\udda5\ufe0f \u2502             \u2502\n\u2502                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                                                                 \u2502\n\u2502   The algorithm discovered \"sports\" vs \"electronics\"           \u2502\n\u2502   without anyone telling it those categories exist!            \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### \ud83d\udd0d Supervised vs Unsupervised: Side by Side\n\n| Aspect | Supervised | Unsupervised |\n|--------|------------|--------------|\n| **Analogy** | Learning with answer key | Exploring without instructions |\n| **Data needed** | Features + Labels | Features only |\n| **Goal** | Predict known categories | Discover hidden patterns |\n| **Example** | \"Is this malware?\" | \"What groups exist in my data?\" |\n| **Use when** | You know what to look for | You don't know what's there |\n\n### \ud83c\udfaf Security Use Cases for Unsupervised Learning\n\n1. **Threat Hunting**: Group similar network connections \u2192 find anomalies\n2. **Malware Families**: Cluster samples \u2192 discover new variants\n3. **User Behavior**: Group activity patterns \u2192 detect compromised accounts\n4. **Log Analysis**: Find unusual log patterns you didn't know to look for"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate unlabeled data\n",
    "np.random.seed(42)\n",
    "cluster1 = np.random.randn(30, 2) + [0, 0]\n",
    "cluster2 = np.random.randn(30, 2) + [4, 4]\n",
    "cluster3 = np.random.randn(30, 2) + [0, 4]\n",
    "data = np.vstack([cluster1, cluster2, cluster3])\n",
    "\n",
    "# Apply clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(data)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=\"viridis\", alpha=0.7)\n",
    "plt.scatter(\n",
    "    kmeans.cluster_centers_[:, 0],\n",
    "    kmeans.cluster_centers_[:, 1],\n",
    "    c=\"red\",\n",
    "    marker=\"X\",\n",
    "    s=200,\n",
    "    label=\"Centers\",\n",
    ")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Unsupervised Learning: Clustering\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Features and Labels\n",
    "\n",
    "**Features (X):** Input variables the model learns from\n",
    "- Email: word count, URL count, sender domain\n",
    "- Malware: file size, imports, entropy\n",
    "- Network: bytes sent, packet count, duration\n",
    "\n",
    "**Labels (y):** What we want to predict\n",
    "- Classification: phishing/legitimate, malware/benign\n",
    "- Regression: threat score (0-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example feature table\n",
    "data = {\n",
    "    \"email_length\": [150, 500, 200, 1000],\n",
    "    \"url_count\": [5, 1, 8, 0],\n",
    "    \"urgent_words\": [3, 0, 5, 1],\n",
    "    \"label\": [\"phishing\", \"legitimate\", \"phishing\", \"legitimate\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Features (X) and Labels (y):\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split\n",
    "\n",
    "**Why split data?**\n",
    "- Training set: Model learns from this\n",
    "- Test set: Evaluate on unseen data\n",
    "- Prevents overfitting (memorizing instead of learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.randn(100, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### \u26a0\ufe0f The Danger of Overfitting: Memorizing vs Learning\n\n**Overfitting** is when a model memorizes the training data instead of learning general patterns. It's like a student who memorizes test answers without understanding the concepts.\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    OVERFITTING: The Core Problem                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502   GOOD MODEL (Generalization)         BAD MODEL (Overfitting)               \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500              \u2502\n\u2502                                                                             \u2502\n\u2502   Training: 95% accuracy              Training: 99.9% accuracy              \u2502\n\u2502   Test:     93% accuracy              Test:     60% accuracy                \u2502\n\u2502                                                                             \u2502\n\u2502   \u2705 Similar performance =            \u274c Huge gap = PROBLEM!                \u2502\n\u2502      learned real patterns                memorized training data           \u2502\n\u2502                                                                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502   \ud83d\udcda ANALOGY: The Cheating Student                                          \u2502\n\u2502                                                                             \u2502\n\u2502   Imagine two students studying for a phishing detection test:              \u2502\n\u2502                                                                             \u2502\n\u2502   Student A (Good model):                                                   \u2502\n\u2502   \"I notice phishing emails often have urgency, suspicious links,          \u2502\n\u2502    and grammar mistakes. I'll look for those patterns.\"                    \u2502\n\u2502   \u2192 Does well on practice tests AND the real exam \u2705                       \u2502\n\u2502                                                                             \u2502\n\u2502   Student B (Overfitting):                                                  \u2502\n\u2502   \"I'll memorize every exact practice question word-for-word.\"             \u2502\n\u2502   \u2192 Perfect on practice tests, fails the real exam \u274c                      \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### \ud83c\udfaf Why This Matters for Security\n\nIn security ML, overfitting is **dangerous**:\n\n| Scenario | What happens with overfitting |\n|----------|------------------------------|\n| Malware detection | Model memorizes training malware signatures, misses new variants |\n| Phishing detection | Model memorizes specific phishing emails, misses new campaigns |\n| Intrusion detection | Model learns training attack patterns, misses novel attacks |\n\n**The solution**: Always test on data the model has never seen (the test set)!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visual demonstration of overfitting vs good fit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\nnp.random.seed(42)\n\n# Generate some noisy data (imagine: file_size vs threat_score)\nX_demo = np.linspace(0, 10, 15).reshape(-1, 1)\ny_demo = 2 * np.sin(X_demo).ravel() + np.random.normal(0, 0.5, 15)  # True pattern + noise\n\n# Create test points\nX_test_demo = np.linspace(0, 10, 100).reshape(-1, 1)\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\n# Model 1: Underfitting (too simple - straight line)\nmodel_simple = LinearRegression()\nmodel_simple.fit(X_demo, y_demo)\ny_simple = model_simple.predict(X_test_demo)\n\naxes[0].scatter(X_demo, y_demo, c='blue', s=60, label='Training data', zorder=5)\naxes[0].plot(X_test_demo, y_simple, 'r-', linewidth=2, label='Model prediction')\naxes[0].set_title('\u274c UNDERFITTING\\n(Too simple - misses pattern)', fontsize=11)\naxes[0].set_xlabel('Feature (e.g., file size)')\naxes[0].set_ylabel('Target (e.g., threat score)')\naxes[0].legend(fontsize=8)\naxes[0].grid(True, alpha=0.3)\n\n# Model 2: Good fit (just right)\nmodel_good = make_pipeline(PolynomialFeatures(3), LinearRegression())\nmodel_good.fit(X_demo, y_demo)\ny_good = model_good.predict(X_test_demo)\n\naxes[1].scatter(X_demo, y_demo, c='blue', s=60, label='Training data', zorder=5)\naxes[1].plot(X_test_demo, y_good, 'g-', linewidth=2, label='Model prediction')\naxes[1].set_title('\u2705 GOOD FIT\\n(Captures pattern, ignores noise)', fontsize=11)\naxes[1].set_xlabel('Feature (e.g., file size)')\naxes[1].legend(fontsize=8)\naxes[1].grid(True, alpha=0.3)\n\n# Model 3: Overfitting (too complex - memorizes noise)\nmodel_overfit = make_pipeline(PolynomialFeatures(12), LinearRegression())\nmodel_overfit.fit(X_demo, y_demo)\ny_overfit = model_overfit.predict(X_test_demo)\n\naxes[2].scatter(X_demo, y_demo, c='blue', s=60, label='Training data', zorder=5)\naxes[2].plot(X_test_demo, y_overfit, 'orange', linewidth=2, label='Model prediction')\naxes[2].set_title('\u274c OVERFITTING\\n(Memorizes noise, fails on new data)', fontsize=11)\naxes[2].set_xlabel('Feature (e.g., file size)')\naxes[2].legend(fontsize=8)\naxes[2].grid(True, alpha=0.3)\naxes[2].set_ylim(-4, 4)  # Limit y-axis to show the wild oscillations\n\nplt.tight_layout()\nplt.suptitle('The Goldilocks Problem: Finding the Right Model Complexity', y=1.02, fontsize=13)\nplt.show()\n\nprint(\"\\n\ud83d\udcd6 What You're Seeing:\")\nprint(\"=\" * 60)\nprint(\"  \u2022 Blue dots: Training data (what the model learned from)\")\nprint(\"  \u2022 Lines: What the model predicts\")\nprint()\nprint(\"  LEFT (Underfitting): Model too simple\")\nprint(\"    \u2192 Misses the underlying pattern entirely\")\nprint(\"    \u2192 Bad on training data AND test data\")\nprint()\nprint(\"  MIDDLE (Good Fit): Model complexity just right\")\nprint(\"    \u2192 Captures the real pattern\")\nprint(\"    \u2192 Ignores random noise in training data\")\nprint(\"    \u2192 Works well on NEW data!\")\nprint()\nprint(\"  RIGHT (Overfitting): Model too complex\")  \nprint(\"    \u2192 Perfectly fits every training point (including noise!)\")\nprint(\"    \u2192 Wild predictions between training points\")\nprint(\"    \u2192 Fails badly on new data\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n",
    "\n",
    "For classification:\n",
    "- **Accuracy**: % correct predictions\n",
    "- **Precision**: Of predicted positives, how many are correct?\n",
    "- **Recall**: Of actual positives, how many did we find?\n",
    "- **F1 Score**: Balance of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Example predictions\n",
    "y_true = [1, 1, 1, 0, 0, 0, 1, 0, 1, 0]  # Actual labels\n",
    "y_pred = [1, 1, 0, 0, 0, 1, 1, 0, 1, 0]  # Model predictions\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malicious\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Security Context: Why Metrics Matter\n",
    "\n",
    "**High Recall needed when:**\n",
    "- Missing a threat is costly (malware detection)\n",
    "- Better to have false alarms than miss attacks\n",
    "\n",
    "**High Precision needed when:**\n",
    "- False positives are expensive (blocking legitimate users)\n",
    "- Alert fatigue is a concern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of precision vs recall tradeoff\n",
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "precision = [0.5, 0.55, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.98]\n",
    "recall = [0.98, 0.95, 0.9, 0.85, 0.75, 0.65, 0.5, 0.35, 0.2]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precision, \"b-o\", label=\"Precision\")\n",
    "plt.plot(thresholds, recall, \"r-o\", label=\"Recall\")\n",
    "plt.xlabel(\"Detection Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision vs Recall Tradeoff\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Supervised**: Learn from labeled examples\n",
    "2. **Unsupervised**: Find patterns without labels\n",
    "3. **Features**: Input data the model uses\n",
    "4. **Train/Test Split**: Evaluate on unseen data\n",
    "5. **Metrics**: Choose based on security context\n",
    "\n",
    "## Next Steps\n",
    "- **Lab 31**: Prompt Engineering Mastery\n",
    "- **Lab 29**: Build your first classifier!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}