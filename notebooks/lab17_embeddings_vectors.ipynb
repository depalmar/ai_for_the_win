{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 17: Embeddings & Vectors Explained\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/depalmar/ai_for_the_win/blob/main/notebooks/lab17_embeddings_vectors.ipynb)\n\nUnderstand how AI \"understands\" meaning through vector representations.\n\n## Learning Objectives\n- Understand what embeddings are and why they matter\n- Create and visualize text embeddings\n- Measure similarity between security concepts\n- Build a simple semantic search system\n\n**Next:** Lab 06 (Security RAG)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies (Colab only)\n",
    "#@markdown Run this cell to install required packages in Colab\n",
    "\n",
    "%pip install -q sentence-transformers scikit-learn numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\u2705 Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Vectors\n",
    "\n",
    "Before we use real embeddings, let's understand vectors with simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: Representing security concepts as 2D vectors\n",
    "# (Real embeddings have 384-1536 dimensions, but principle is same)\n",
    "\n",
    "# Let's say our two dimensions are:\n",
    "# Dimension 1: How \"attack-related\" (0 = defensive, 1 = offensive)\n",
    "# Dimension 2: How \"network-related\" (0 = host, 1 = network)\n",
    "\n",
    "security_concepts = {\n",
    "    \"malware\":       [0.9, 0.2],  # Offensive, host-focused\n",
    "    \"ransomware\":    [0.95, 0.1], # Very offensive, host-focused\n",
    "    \"phishing\":      [0.8, 0.6],  # Offensive, some network\n",
    "    \"firewall\":      [0.1, 0.9],  # Defensive, network-focused\n",
    "    \"antivirus\":     [0.1, 0.2],  # Defensive, host-focused\n",
    "    \"IDS\":           [0.2, 0.95], # Defensive, network-focused\n",
    "    \"C2_traffic\":    [0.9, 0.9],  # Offensive, network-focused\n",
    "}\n",
    "\n",
    "print(\"Security Concepts as 2D Vectors:\")\n",
    "print(\"-\" * 50)\n",
    "for concept, vector in security_concepts.items():\n",
    "    print(f\"  {concept:15} \u2192 {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Measuring Similarity with Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the angle between two vectors:\n",
    "- **1.0** = Identical direction (same meaning)\n",
    "- **0.0** = Perpendicular (unrelated)\n",
    "- **-1.0** = Opposite direction (opposite meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity between security concepts\n",
    "\n",
    "def get_similarity(concept1: str, concept2: str) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two concepts.\"\"\"\n",
    "    vec1 = np.array([security_concepts[concept1]])\n",
    "    vec2 = np.array([security_concepts[concept2]])\n",
    "    return cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "# Compare some pairs\n",
    "comparisons = [\n",
    "    (\"malware\", \"ransomware\"),     # Should be very similar\n",
    "    (\"firewall\", \"IDS\"),           # Should be similar (both defensive)\n",
    "    (\"malware\", \"antivirus\"),      # Should be different (opposite purposes)\n",
    "    (\"phishing\", \"C2_traffic\"),    # Both attacks, somewhat similar\n",
    "]\n",
    "\n",
    "print(\"Similarity Comparisons:\")\n",
    "print(\"-\" * 60)\n",
    "for c1, c2 in comparisons:\n",
    "    sim = get_similarity(c1, c2)\n",
    "    interpretation = \"Very similar\" if sim > 0.8 else \"Similar\" if sim > 0.5 else \"Different\"\n",
    "    print(f\"  {c1:15} vs {c2:15} = {sim:.3f}  ({interpretation})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Using Real Embeddings (sentence-transformers)\n",
    "\n",
    "Now let's use a real embedding model that captures semantic meaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real embedding model\n",
    "# all-MiniLM-L6-v2 is fast, free, and good for learning\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Loading embedding model (may take a moment)...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"\u2705 Model loaded! Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security-focused text for embedding comparison\n",
    "security_texts = [\n",
    "    \"Malware using PowerShell for execution\",\n",
    "    \"Ransomware encrypting files on the system\",\n",
    "    \"Phishing email with malicious attachment\",\n",
    "    \"Attacker stealing passwords from memory\",\n",
    "    \"Quarterly financial report for Q4 2024\",\n",
    "    \"Team meeting scheduled for Monday\",\n",
    "    \"C2 beacon establishing connection\",\n",
    "    \"Mimikatz used to dump credentials\",\n",
    "]\n",
    "\n",
    "# Create embeddings for all texts\n",
    "print(\"Creating embeddings for security texts...\")\n",
    "embeddings = model.encode(security_texts)\n",
    "\n",
    "print(f\"\\\\n\u2705 Created {len(embeddings)} embeddings\")\n",
    "print(f\"   Each embedding has {len(embeddings[0])} dimensions\")\n",
    "print(f\"   First 5 values of first embedding: {embeddings[0][:5].round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## \ud83d\udd2c What's Actually INSIDE an Embedding?\n\nThis is the question everyone wonders about but few explain well!\n\n### The Short Answer\n\nEach dimension captures **some aspect of meaning**, but:\n- We don't know exactly what each dimension represents\n- The model learned these representations from millions of examples\n- They're not human-interpretable like \"dimension 1 = attack-related\"\n\n### The Longer Answer: How Embeddings Are Trained\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    HOW EMBEDDING MODELS LEARN                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  TRAINING DATA: Millions of text examples                                   \u2502\n\u2502                                                                             \u2502\n\u2502  \"The malware encrypted files using AES-256\"                               \u2502\n\u2502  \"Ransomware typically encrypts data and demands payment\"                  \u2502\n\u2502  \"The firewall blocked suspicious outbound traffic\"                        \u2502\n\u2502  ... millions more ...                                                      \u2502\n\u2502                                                                             \u2502\n\u2502  TRAINING PROCESS:                                                          \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                                         \u2502\n\u2502  The model learns: \"malware\" often appears near \"encrypt\", \"file\",         \u2502\n\u2502  \"execute\", \"payload\" \u2192 these should have SIMILAR vectors                  \u2502\n\u2502                                                                             \u2502\n\u2502  The model also learns: \"malware\" rarely appears with \"quarterly\",          \u2502\n\u2502  \"meeting\", \"vacation\" \u2192 these should have DIFFERENT vectors               \u2502\n\u2502                                                                             \u2502\n\u2502  RESULT: Similar concepts end up in similar locations in vector space!     \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Analogy: The Map of Meaning\n\nThink of embeddings as GPS coordinates on a **map of meaning**:\n\n```\n            Network-focused\n                  \u2191\n                  \u2502    \ud83d\udd25 DDoS attack\n                  \u2502    \ud83d\udd25 C2 traffic\n                  \u2502    \ud83d\udee1\ufe0f Firewall\n                  \u2502    \ud83d\udee1\ufe0f IDS\n Defensive \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Offensive\n                  \u2502    \ud83d\udee1\ufe0f Antivirus\n                  \u2502    \ud83d\udd25 Ransomware\n                  \u2502    \ud83d\udd25 Malware\n                  \u2502\n                  \u2193\n             Host-focused\n\nReal embeddings have 384+ dimensions, capturing:\n- Topic (security vs business vs personal)\n- Sentiment (threat vs benign)\n- Specificity (general vs technical)\n- Entity types (tool vs technique vs actor)\n- And hundreds more nuances we can't name!\n```\n\n### What The 384 Dimensions Might Capture\n\n| Dimension Range | Possible Meaning (speculative) |\n|-----------------|-------------------------------|\n| Dims 1-50 | Basic syntax and structure |\n| Dims 50-150 | Topic and domain (security, finance, etc.) |\n| Dims 150-250 | Sentiment and intent |\n| Dims 250-350 | Entity relationships |\n| Dims 350-384 | Fine-grained distinctions |\n\n**Important**: These are educated guesses! The actual dimensions are learned, not designed.\n\n### Why This Matters for Security\n\nThe magic is that **you don't need to understand every dimension** - you just need to know that:\n\n1. Similar threats \u2192 similar embeddings \u2192 easy to find\n2. Novel threats \u2192 unusual embeddings \u2192 anomaly detection\n3. Related IOCs \u2192 close embeddings \u2192 threat correlation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Similarity Matrix (security vs business texts):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show select comparisons\n",
    "test_pairs = [\n",
    "    (0, 1),  # Malware vs Ransomware\n",
    "    (3, 7),  # Password stealing vs Mimikatz (same concept!)\n",
    "    (0, 4),  # Malware vs Quarterly report (unrelated)\n",
    "    (2, 6),  # Phishing vs C2 beacon (both attacks)\n",
    "    (4, 5),  # Business texts (both non-security)\n",
    "]\n",
    "\n",
    "for i, j in test_pairs:\n",
    "    sim = similarity_matrix[i][j]\n",
    "    interpretation = \"\ud83d\udfe2 Similar\" if sim > 0.5 else \"\ud83d\udd34 Different\"\n",
    "    print(f\"{interpretation} ({sim:.3f})\")\n",
    "    print(f\"   '{security_texts[i][:40]}...'\")\n",
    "    print(f\"   '{security_texts[j][:40]}...'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Building Semantic Search\n",
    "\n",
    "The real power: find related content by meaning, not just keywords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, documents: List[str], top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Find most similar documents to a query using embeddings.\n",
    "\n",
    "    This is the foundation of RAG (Retrieval Augmented Generation)!\n",
    "\n",
    "    Args:\n",
    "        query: Search query\n",
    "        documents: List of documents to search\n",
    "        top_k: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        List of (document, similarity) tuples\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Embed all documents\n",
    "    doc_embeddings = model.encode(documents)\n",
    "\n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    return [(documents[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "# Test semantic search\n",
    "threat_docs = [\n",
    "    \"The attacker used Mimikatz to extract credentials from LSASS memory\",\n",
    "    \"Ransomware encrypted all files with .locked extension\",\n",
    "    \"Phishing email contained a malicious Word document with macros\",\n",
    "    \"Lateral movement detected via PsExec to multiple hosts\",\n",
    "    \"C2 communication established over DNS tunneling\",\n",
    "    \"Quarterly financial results exceeded expectations\",\n",
    "]\n",
    "\n",
    "print(\"\ud83d\udd0d SEMANTIC SEARCH DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"attacker stealing passwords\"\n",
    "print(f\"\\\\nQuery: '{query}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results = semantic_search(query, threat_docs)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. ({score:.3f}) {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Key Takeaways\n",
    "\n",
    "1. **Embeddings capture meaning** - Similar text \u2192 similar vectors\n",
    "2. **Cosine similarity** - Standard way to compare embeddings (0-1)\n",
    "3. **Semantic search** - Find by meaning, not exact words\n",
    "4. **Dimension matters** - More dimensions = more nuance, but slower\n",
    "5. **Foundation for RAG** - Embeddings power retrieval in RAG systems\n",
    "\n",
    "### Similarity Score Guide\n",
    "\n",
    "```\n",
    "1.0  = Identical meaning\n",
    "0.8+ = Very similar (synonyms, same topic)\n",
    "0.5-0.8 = Related\n",
    "0.3-0.5 = Loosely related\n",
    "<0.3 = Unrelated\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Lab 06**: Build a full RAG system with ChromaDB\n",
    "- **Lab 07**: Use embeddings to find similar malware patterns\n",
    "- **Lab 16**: Use embeddings for threat actor clustering"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
